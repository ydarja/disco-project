{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the support of relations in train and dev and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file explores how many occurrences of each relation (support) are in the train, dev and test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents:\n",
    "- 00) Setup\n",
    "- 01) Get the file paths\n",
    "- 02) Data preprocessing function\n",
    "- 03) Flatten data\n",
    "- 04) Get support of labels in the flattened data\n",
    "- 05) Genre document and edu pair count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.probability import FreqDist\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_rsd_file_paths(directory): \n",
    "    \n",
    "    try: \n",
    "        # Get all file names in the specified directory \n",
    "        file_names = os.listdir(directory) \n",
    "\n",
    "        # Make sure to only capture .rsd files\n",
    "        rs4_file_paths = [directory + '/' + file for file in file_names if file.endswith('.rsd')] \n",
    "\n",
    "        return rs4_file_paths \n",
    "\n",
    "    # Error handling\n",
    "    except Exception as e: \n",
    "\n",
    "        print(f'An error occurred: {e}') \n",
    "\n",
    "        return [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rsd files in the train & dev & test file folders\n",
    "train_rsd_file_paths = list_rsd_file_paths('C:/Users/marco/OneDrive/24_25_WS/Discourse_modeling_and_processing/disco-project/data/train') \n",
    "dev_rsd_file_paths = list_rsd_file_paths('C:/Users/marco/OneDrive/24_25_WS/Discourse_modeling_and_processing/disco-project/data/dev')\n",
    "test_rsd_file_paths = list_rsd_file_paths('C:/Users/marco/OneDrive/24_25_WS/Discourse_modeling_and_processing/disco-project/data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rsd_file_paths_to_dict(rsd_file_paths, fine_grained=True):\n",
    "\n",
    "    group_genre_file_dict = {\n",
    "        'cluster0':{\n",
    "            'bio':{},\n",
    "            'fiction':{}\n",
    "        },\n",
    "        'cluster1':{\n",
    "            'academic':{},\n",
    "            'interview':{},\n",
    "            'letter':{},\n",
    "            'news':{},\n",
    "            'speech':{},\n",
    "            'textbook':{},\n",
    "            'voyage':{}\n",
    "        },\n",
    "        'cluster2':{\n",
    "            'conversation':{},\n",
    "        },\n",
    "        'cluster3':{\n",
    "            'court':{},\n",
    "            'essay':{},\n",
    "            'podcast':{},\n",
    "            'reddit':{},\n",
    "            'vlog':{}\n",
    "        },\n",
    "        'cluster4':{\n",
    "            'whow':{}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    labels = []\n",
    "\n",
    "    for file_path in rsd_file_paths:\n",
    "\n",
    "        ids = []\n",
    "        texts = []\n",
    "        parents = []\n",
    "        relations = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as file: \n",
    "            for line in file:\n",
    "\n",
    "                row = line.split('\\t')\n",
    "\n",
    "                try:\n",
    "                    ids.append(int(row[0]))\n",
    "                    texts.append(row[1])\n",
    "                    parents.append(int(row[6]))\n",
    "                    relations.append(row[7])\n",
    "\n",
    "                except IndexError: \n",
    "                    print(f\"Skipping row with insufficient columns in file: {file_path}\")\n",
    "                    ids = ids[:len(relations)]\n",
    "                    texts = texts[:len(relations)]\n",
    "                    parents = parents[:len(relations)]\n",
    "\n",
    "        edu_pairs = []\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "            if relations[i][-1] == 'r' and parents[i] in ids:\n",
    "\n",
    "                edu_text = ['<s>'] + texts[i].split(' ') + ['<sep>'] + texts[ids.index(parents[i])].split(' ') + ['<n>']\n",
    "\n",
    "                if fine_grained:\n",
    "                    edu_pairs.append([edu_text, relations[i][:-2]])\n",
    "                    labels.append(relations[i][:-2])\n",
    "                else: # coarse\n",
    "                    edu_pairs.append([edu_text, relations[i][:relations[i].rfind('-')] if relations[i].find('same') else relations[i][:-2]])\n",
    "                    labels.append(relations[i][:relations[i].rfind('-')] if relations[i].find('same') else relations[i][:-2])\n",
    "\n",
    "            elif relations[i][-1] == 'm' and parents[i] in ids:\n",
    "                edu_text = ['<n>'] + texts[i].split(' ') + ['<sep>'] + texts[ids.index(parents[i])].split(' ') + ['<n>']\n",
    "\n",
    "                if fine_grained:\n",
    "                    edu_pairs.append([edu_text, relations[i][:-2]])\n",
    "                    labels.append(relations[i][:-2])\n",
    "                else: # coarse\n",
    "                    edu_pairs.append([edu_text, relations[i][:relations[i].rfind('-')] if relations[i].find('same') else relations[i][:-2]])\n",
    "                    labels.append(relations[i][:relations[i].rfind('-')] if relations[i].find('same') else relations[i][:-2])\n",
    "                    \n",
    "        if file_path.find('/') >= 0:\n",
    "            shortened_file_path = file_path[file_path.rfind('/')+1:]\n",
    "            file_genre = shortened_file_path[shortened_file_path.find('_')+1:shortened_file_path.rfind('_')]\n",
    "            file_document = shortened_file_path[shortened_file_path.rfind('_')+1:shortened_file_path.find('.')]\n",
    "        else:\n",
    "            file_genre = file_path[file_path.find('_')+1:file_path.rfind('_')]\n",
    "            file_document = file_path[file_path.rfind('_')+1:file_path.find('.')]\n",
    "\n",
    "\n",
    "        if file_genre in ['bio', 'fiction']:\n",
    "            group_genre_file_dict['cluster0'][file_genre][file_document] = edu_pairs\n",
    "            \n",
    "        elif file_genre in ['academic', 'interview', 'letter', 'news', 'speech', 'textbook', 'voyage']:\n",
    "            group_genre_file_dict['cluster1'][file_genre][file_document] = edu_pairs\n",
    "\n",
    "        elif file_genre in ['conversation']:\n",
    "            group_genre_file_dict['cluster2'][file_genre][file_document] = edu_pairs\n",
    "\n",
    "        elif file_genre in ['court', 'essay', 'podcast', 'reddit', 'vlog']:\n",
    "            group_genre_file_dict['cluster3'][file_genre][file_document] = edu_pairs\n",
    "        \n",
    "        elif file_genre in ['whow']:\n",
    "            group_genre_file_dict['cluster4'][file_genre][file_document] = edu_pairs\n",
    "\n",
    "        else:\n",
    "            print(f\"{file_document} of {file_genre} could not be assigned to the output dictionary!\")\n",
    "       \n",
    "    return group_genre_file_dict, set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the preprocessed data for train & dev & test with \n",
    "# Coarse labels\n",
    "train_coarse_labelled_data, _ = rsd_file_paths_to_dict(train_rsd_file_paths, fine_grained=False)\n",
    "dev_coarse_labelled_data, _ = rsd_file_paths_to_dict(dev_rsd_file_paths, fine_grained=False)\n",
    "test_coarse_labelled_data, _ = rsd_file_paths_to_dict(test_rsd_file_paths, fine_grained=False)\n",
    "\n",
    "# Fine grained labels\n",
    "train_fine_grained_labelled_data, _ = rsd_file_paths_to_dict(train_rsd_file_paths, fine_grained=True)\n",
    "dev_fine_grained_labelled_data, _ = rsd_file_paths_to_dict(dev_rsd_file_paths, fine_grained=True)\n",
    "test_fine_grained_labelled_data, _ = rsd_file_paths_to_dict(test_rsd_file_paths, fine_grained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Flatten data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_data(group_genre_file_dict):\n",
    "\n",
    "    # Flatten the data into a list of EDU pairs and relations\n",
    "    edu_pairs_list = []\n",
    "    for group in group_genre_file_dict.values():\n",
    "        for genre in group.values():\n",
    "            for document in genre.values():\n",
    "                edu_pairs_list.extend(document)\n",
    "\n",
    "    return edu_pairs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the preprocessed data as a flat list of EDU pairs with relations\n",
    "# Coarse labels\n",
    "train_coarse_labelled_data_flat = flatten_data(train_coarse_labelled_data)\n",
    "dev_coarse_labelled_data_flat = flatten_data(dev_coarse_labelled_data)\n",
    "test_coarse_labelled_data_flat = flatten_data(test_coarse_labelled_data)\n",
    "\n",
    "# Fine grained labels\n",
    "train_fine_grained_labelled_data_flat = flatten_data(train_fine_grained_labelled_data)\n",
    "dev_fine_grained_labelled_data_flat = flatten_data(dev_fine_grained_labelled_data)\n",
    "test_fine_grained_labelled_data_flat = flatten_data(test_fine_grained_labelled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Get support of labels in the flattened data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation_support(flattened_data, output_directory_file):\n",
    "    \n",
    "    # Create a frequency distribution\n",
    "    fdist = FreqDist([relation for _, relation in flattened_data]) \n",
    "\n",
    "    # Convert the frequency distribution to a DataFrame\n",
    "    df = pd.DataFrame(list(fdist.items()), columns=['Relation', 'Support'])\n",
    "\n",
    "    # Sort the DataFrame alphabetically by the 'Relation' column\n",
    "    df_sorted = df.sort_values(by='Relation')\n",
    "\n",
    "    # Save the sorted DataFrame to a CSV file in the result folder\n",
    "    df_sorted.to_csv(output_directory_file, index=False)\n",
    "\n",
    "    print(df_sorted)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data and coarse labels:\n",
      "        Relation  Support\n",
      "5    adversative     1541\n",
      "13   attribution     1421\n",
      "8         causal      954\n",
      "4        context     1966\n",
      "14   contingency      423\n",
      "1    elaboration     4378\n",
      "9     evaluation      904\n",
      "7    explanation     1361\n",
      "2          joint     4533\n",
      "11          mode      407\n",
      "0   organization     1715\n",
      "6        purpose      787\n",
      "10   restatement      756\n",
      "3      same-unit     1222\n",
      "12         topic      443\n",
      "\n",
      "Dev data and coarse labels:\n",
      "        Relation  Support\n",
      "5    adversative      272\n",
      "8    attribution      200\n",
      "7         causal      171\n",
      "3        context      270\n",
      "14   contingency       65\n",
      "4    elaboration      610\n",
      "11    evaluation      150\n",
      "2    explanation      235\n",
      "1          joint      639\n",
      "12          mode       67\n",
      "0   organization      196\n",
      "6        purpose      115\n",
      "10   restatement      124\n",
      "9      same-unit      179\n",
      "13         topic       70\n",
      "\n",
      "Test data and coarse labels:\n",
      "        Relation  Support\n",
      "10   adversative      220\n",
      "12   attribution      189\n",
      "5         causal      123\n",
      "9        context      288\n",
      "13   contingency       68\n",
      "2    elaboration      710\n",
      "7     evaluation      140\n",
      "8    explanation      217\n",
      "6          joint      661\n",
      "11          mode       64\n",
      "0   organization      213\n",
      "4        purpose      126\n",
      "1    restatement       66\n",
      "3      same-unit      167\n",
      "14         topic       49\n",
      "\n",
      "Train data and fine grained labels:\n",
      "                    Relation  Support\n",
      "15    adversative-antithesis      380\n",
      "8     adversative-concession      740\n",
      "16      adversative-contrast      421\n",
      "29      attribution-negative       82\n",
      "24      attribution-positive     1339\n",
      "14              causal-cause      549\n",
      "17             causal-result      405\n",
      "6         context-background     1042\n",
      "7       context-circumstance      924\n",
      "25     contingency-condition      423\n",
      "2     elaboration-additional     2246\n",
      "5      elaboration-attribute     2132\n",
      "18        evaluation-comment      904\n",
      "13      explanation-evidence      662\n",
      "20       explanation-justify      477\n",
      "27    explanation-motivation      222\n",
      "26         joint-disjunction      177\n",
      "3                 joint-list     2062\n",
      "12               joint-other     1125\n",
      "10            joint-sequence     1169\n",
      "21               mode-manner      255\n",
      "23                mode-means      152\n",
      "0       organization-heading      369\n",
      "30       organization-phatic      688\n",
      "1   organization-preparation      658\n",
      "11         purpose-attribute      288\n",
      "9               purpose-goal      499\n",
      "19       restatement-partial      380\n",
      "28    restatement-repetition      376\n",
      "4                  same-unit     1222\n",
      "31            topic-question      365\n",
      "22        topic-solutionhood       78\n",
      "\n",
      "Dev data and fine grained labels:\n",
      "                    Relation  Support\n",
      "26    adversative-antithesis       67\n",
      "11    adversative-concession      130\n",
      "5       adversative-contrast       75\n",
      "24      attribution-negative       11\n",
      "14      attribution-positive      189\n",
      "13              causal-cause       98\n",
      "8              causal-result       73\n",
      "17        context-background      149\n",
      "3       context-circumstance      121\n",
      "31     contingency-condition       65\n",
      "4     elaboration-additional      294\n",
      "9      elaboration-attribute      316\n",
      "19        evaluation-comment      150\n",
      "2       explanation-evidence      139\n",
      "25       explanation-justify       70\n",
      "23    explanation-motivation       26\n",
      "27         joint-disjunction       30\n",
      "10                joint-list      285\n",
      "7                joint-other      177\n",
      "1             joint-sequence      147\n",
      "21               mode-manner       51\n",
      "20                mode-means       16\n",
      "0       organization-heading       37\n",
      "18       organization-phatic       72\n",
      "12  organization-preparation       87\n",
      "6          purpose-attribute       39\n",
      "22              purpose-goal       76\n",
      "16       restatement-partial       44\n",
      "28    restatement-repetition       80\n",
      "15                 same-unit      179\n",
      "30            topic-question       64\n",
      "29        topic-solutionhood        6\n",
      "\n",
      "Test data and fine grained labels:\n",
      "                    Relation  Support\n",
      "19    adversative-antithesis       61\n",
      "16    adversative-concession       90\n",
      "15      adversative-contrast       69\n",
      "30      attribution-negative       10\n",
      "21      attribution-positive      179\n",
      "7               causal-cause       63\n",
      "17             causal-result       60\n",
      "12        context-background      159\n",
      "14      context-circumstance      129\n",
      "25     contingency-condition       68\n",
      "4     elaboration-additional      325\n",
      "10     elaboration-attribute      385\n",
      "9         evaluation-comment      140\n",
      "11      explanation-evidence      115\n",
      "24       explanation-justify       77\n",
      "28    explanation-motivation       25\n",
      "27         joint-disjunction       25\n",
      "18                joint-list      338\n",
      "8                joint-other      147\n",
      "13            joint-sequence      151\n",
      "23               mode-manner       39\n",
      "20                mode-means       25\n",
      "0       organization-heading       50\n",
      "31       organization-phatic       64\n",
      "2   organization-preparation       99\n",
      "6          purpose-attribute       46\n",
      "22              purpose-goal       80\n",
      "1        restatement-partial       35\n",
      "3     restatement-repetition       31\n",
      "5                  same-unit      167\n",
      "26            topic-question       41\n",
      "29        topic-solutionhood        8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the support of relations in the flattened data\n",
    "# Coarse labels\n",
    "print(\"Train data and coarse labels:\")\n",
    "get_relation_support(train_coarse_labelled_data_flat, 'results/train_coarse_labelled_support.csv')\n",
    "print(\"\")\n",
    "print(\"Dev data and coarse labels:\")\n",
    "get_relation_support(dev_coarse_labelled_data_flat, 'results/dev_coarse_labelled_support.csv')\n",
    "print(\"\")\n",
    "print(\"Test data and coarse labels:\")\n",
    "get_relation_support(test_coarse_labelled_data_flat, 'results/test_coarse_labelled_support.csv')\n",
    "print(\"\")\n",
    "\n",
    "# Fine grained labels\n",
    "print(\"Train data and fine grained labels:\")\n",
    "get_relation_support(train_fine_grained_labelled_data_flat, 'results/train_fine_grained_labelled_support.csv')\n",
    "print(\"\")\n",
    "print(\"Dev data and fine grained labels:\")\n",
    "get_relation_support(dev_fine_grained_labelled_data_flat, 'results/dev_fine_grained_labelled_support.csv')\n",
    "print(\"\")\n",
    "print(\"Test data and fine grained labels:\")\n",
    "get_relation_support(test_fine_grained_labelled_data_flat, 'results/test_fine_grained_labelled_support.csv')\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Genre document and EDU pair count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the documents and EDU pairs per genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the genre statistics dictionaries with the desired key names\n",
    "genre_stats_dict_train = {\n",
    "    'academic': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'bio': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'conversation': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'court': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'essay': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'fiction': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'interview': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'letter': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'news': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'podcast': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'reddit': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'speech': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'textbook': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'vlog': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'voyage': {'train: documents #': 0, 'train: relations #': 0},\n",
    "    'whow': {'train: documents #': 0, 'train: relations #': 0}\n",
    "}\n",
    "\n",
    "genre_stats_dict_dev = {\n",
    "    'academic': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'bio': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'conversation': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'court': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'essay': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'fiction': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'interview': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'letter': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'news': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'podcast': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'reddit': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'speech': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'textbook': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'vlog': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'voyage': {'dev: documents #': 0, 'dev: relations #': 0},\n",
    "    'whow': {'dev: documents #': 0, 'dev: relations #': 0}\n",
    "}\n",
    "\n",
    "genre_stats_dict_test = {\n",
    "    'academic': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'bio': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'conversation': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'court': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'essay': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'fiction': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'interview': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'letter': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'news': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'podcast': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'reddit': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'speech': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'textbook': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'vlog': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'voyage': {'test: documents #': 0, 'test: relations #': 0},\n",
    "    'whow': {'test: documents #': 0, 'test: relations #': 0}\n",
    "}\n",
    "\n",
    "# Update the genre statistics dictionaries based on the training data\n",
    "for group in train_coarse_labelled_data.keys():\n",
    "    for genre in train_coarse_labelled_data[group].keys():\n",
    "        for document in train_coarse_labelled_data[group][genre].keys():\n",
    "            genre_stats_dict_train[genre]['train: documents #'] += 1\n",
    "            genre_stats_dict_train[genre]['train: relations #'] += len(train_coarse_labelled_data[group][genre][document])\n",
    "\n",
    "# Update the genre statistics dictionaries based on the dev data\n",
    "for group in dev_coarse_labelled_data.keys():\n",
    "    for genre in dev_coarse_labelled_data[group].keys():\n",
    "        for document in dev_coarse_labelled_data[group][genre].keys():\n",
    "            genre_stats_dict_dev[genre]['dev: documents #'] += 1\n",
    "            genre_stats_dict_dev[genre]['dev: relations #'] += len(dev_coarse_labelled_data[group][genre][document])\n",
    "\n",
    "# Update the genre statistics dictionaries based on the test data\n",
    "for group in test_coarse_labelled_data.keys():\n",
    "    for genre in test_coarse_labelled_data[group].keys():\n",
    "        for document in test_coarse_labelled_data[group][genre].keys():\n",
    "            genre_stats_dict_test[genre]['test: documents #'] += 1\n",
    "            genre_stats_dict_test[genre]['test: relations #'] += len(test_coarse_labelled_data[group][genre][document])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataframes out of the dictionaries\n",
    " \n",
    "train_data_df = pd.DataFrame(genre_stats_dict_train).transpose().rename_axis('genre').reset_index()\n",
    "\n",
    "dev_data_df = pd.DataFrame(genre_stats_dict_dev).transpose().rename_axis('genre').reset_index()\n",
    "\n",
    "test_data_df = pd.DataFrame(genre_stats_dict_test).transpose().rename_axis('genre').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes together\n",
    "temp_df = pd.merge(train_data_df, dev_data_df, on='genre', how='outer')\n",
    "\n",
    "data_document_relation_df = pd.merge(temp_df, test_data_df, on='genre', how='outer')\n",
    "\n",
    "# Save the result\n",
    "data_document_relation_df.to_csv(\"results/genre_document_relation_frequencies.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>train: documents #</th>\n",
       "      <th>train: relations #</th>\n",
       "      <th>dev: documents #</th>\n",
       "      <th>dev: relations #</th>\n",
       "      <th>test: documents #</th>\n",
       "      <th>test: relations #</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>academic</td>\n",
       "      <td>14</td>\n",
       "      <td>1514</td>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>2</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bio</td>\n",
       "      <td>16</td>\n",
       "      <td>1697</td>\n",
       "      <td>2</td>\n",
       "      <td>174</td>\n",
       "      <td>2</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>conversation</td>\n",
       "      <td>10</td>\n",
       "      <td>2075</td>\n",
       "      <td>2</td>\n",
       "      <td>430</td>\n",
       "      <td>2</td>\n",
       "      <td>340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>court</td>\n",
       "      <td>4</td>\n",
       "      <td>656</td>\n",
       "      <td>1</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>essay</td>\n",
       "      <td>3</td>\n",
       "      <td>370</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fiction</td>\n",
       "      <td>15</td>\n",
       "      <td>1969</td>\n",
       "      <td>2</td>\n",
       "      <td>224</td>\n",
       "      <td>2</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>interview</td>\n",
       "      <td>15</td>\n",
       "      <td>1998</td>\n",
       "      <td>2</td>\n",
       "      <td>186</td>\n",
       "      <td>2</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>letter</td>\n",
       "      <td>4</td>\n",
       "      <td>562</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>news</td>\n",
       "      <td>19</td>\n",
       "      <td>1356</td>\n",
       "      <td>2</td>\n",
       "      <td>202</td>\n",
       "      <td>2</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>podcast</td>\n",
       "      <td>3</td>\n",
       "      <td>517</td>\n",
       "      <td>1</td>\n",
       "      <td>186</td>\n",
       "      <td>1</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>reddit</td>\n",
       "      <td>14</td>\n",
       "      <td>1729</td>\n",
       "      <td>2</td>\n",
       "      <td>261</td>\n",
       "      <td>3</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>speech</td>\n",
       "      <td>11</td>\n",
       "      <td>1463</td>\n",
       "      <td>2</td>\n",
       "      <td>254</td>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>textbook</td>\n",
       "      <td>11</td>\n",
       "      <td>1569</td>\n",
       "      <td>2</td>\n",
       "      <td>190</td>\n",
       "      <td>2</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>vlog</td>\n",
       "      <td>11</td>\n",
       "      <td>1922</td>\n",
       "      <td>2</td>\n",
       "      <td>277</td>\n",
       "      <td>2</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>voyage</td>\n",
       "      <td>14</td>\n",
       "      <td>1470</td>\n",
       "      <td>2</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>whow</td>\n",
       "      <td>15</td>\n",
       "      <td>1944</td>\n",
       "      <td>2</td>\n",
       "      <td>214</td>\n",
       "      <td>2</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           genre  train: documents #  train: relations #  dev: documents #  \\\n",
       "0       academic                  14                1514                 2   \n",
       "1            bio                  16                1697                 2   \n",
       "2   conversation                  10                2075                 2   \n",
       "3          court                   4                 656                 1   \n",
       "4          essay                   3                 370                 1   \n",
       "5        fiction                  15                1969                 2   \n",
       "6      interview                  15                1998                 2   \n",
       "7         letter                   4                 562                 1   \n",
       "8           news                  19                1356                 2   \n",
       "9        podcast                   3                 517                 1   \n",
       "10        reddit                  14                1729                 2   \n",
       "11        speech                  11                1463                 2   \n",
       "12      textbook                  11                1569                 2   \n",
       "13          vlog                  11                1922                 2   \n",
       "14        voyage                  14                1470                 2   \n",
       "15          whow                  15                1944                 2   \n",
       "\n",
       "    dev: relations #  test: documents #  test: relations #  \n",
       "0                202                  2                247  \n",
       "1                174                  2                180  \n",
       "2                430                  2                340  \n",
       "3                128                  1                 94  \n",
       "4                171                  1                146  \n",
       "5                224                  2                262  \n",
       "6                186                  2                207  \n",
       "7                109                  1                103  \n",
       "8                202                  2                198  \n",
       "9                186                  1                109  \n",
       "10               261                  3                388  \n",
       "11               254                  2                182  \n",
       "12               190                  2                253  \n",
       "13               277                  2                222  \n",
       "14               155                  2                149  \n",
       "15               214                  2                221  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_document_relation_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
